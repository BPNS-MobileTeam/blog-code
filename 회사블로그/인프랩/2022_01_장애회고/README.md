# 2022년 1월 100% 할인 이벤트 장애 회고

안녕하세요 인프런의 향로입니다.  
인프런 서비스는 2022년 1월 3일 ~ 17일까지 지속적으로 서비스를 사용할 수 없는, 역대급 장애가 발생습니다.  

이 포스팅을 통해 서버 문제가 발생한 원인과 이를 해소한 과정을 투명하게 공유드리려고 합니다.  
  
조금은 적절한 짤과 함께 재미난 형태로 풀고 싶었지만,  
막상 해당 장애를 통해 불편을 느끼신 분들에게는 오히려 예의가 아닐수도 있다는 생각에 조금은 진지한 형태로 글을 작성하게 되었습니다.  

> 다시 한번 서비스 사용에 불편을 드려 정말 죄송합니다. 

# 1. 발단

2022년을 맞이해서 인프런에서는 [특정 강의를 100% 할인 이벤트](https://www.inflearn.com/pages/start-20220103)를 진행했습니다.  

다양한 분야의 **강의 125개**, **총 5,016,000원** 강의를 **모두 0원**에 아래 기간동안 등록할 수 있는 이벤트였습니다.

* 첫번째 이벤트: 2022년 1월 3일 (월) ~ 2022년 1월 17일 (월)
* 두번째 이벤트: 2022년 1월 18일 (화) ~ 2022년 1월 24일 (월)

![intro](./images/intro.png)

이 이벤트는 저희의 예상을 벗어날정도로 엄청난 바이럴이 발생했는데요.

![twitter1](./images/twitter1.png)

![twitter2](./images/twitter2.png)

트위터에서만 **1만 리트윗이상**이 발생하고, 그 외 디시인사이드, 루리웹, 클리앙등 여러 IT 커뮤니티에서도 공유가 되어 **평소 대비 5~20배의 트래픽**이 발생했습니다.  
  
그 덕에 **7일만에 76,730의 신규 가입자**분들, 전년 대비 **1,855**%라는 놀라운 수치를 달성하기도 했습니다.

![event_result](./images/event_result.png)

반면, 그만큼의 트래픽이 발생했기 때문에 장애가 지속적으로 발생했는데요.  
아래는 **시간별 트랜잭션수** 인데, 빨간박스가 평소대비 높은 수치를 달성한 시간이고 **모두 장애가 발생**하였습니다.

![1_1](./images/1_1.png)

(1월 3일 ~ 1월 8일)

![1_2](./images/1_2.png)

(1월 16일 ~ 1월 21일)  
    
이 수치는 그간 저희 서비스에서 겪지 못했던 수준이였고, 앞선 이벤트들에서도 만나지 못했던 수치였습니다.  
  
그러다보니 1월 3일, 4일, 7일, 17일 등 **총 4일 동안 오전 9시, 오후 10시**가 되면 거의 서비스가 불가능할정도로 불안정한 상태를 유지하였습니다.    
(그래서 **총 6번의 서비스 장애**가 발생했습니다.)  
  
단순하게보면 트래픽을 통한 서비스 장애라고 볼 수 있겠습니다.  
하지만 여기까지 보신 분들이라면 의아한 점이 2가지가 있으실텐데요.

* 아무리 폭증한 트래픽이라하더라도 쿠팡/배민 등에 비하면 작은 트래픽일텐데, 클라우드 서비스의 스케일아웃/스케일업도 안했었나?
* 1월 8일 장애 이후 17일 장애까지는 10일간의 시간이 있는데, 그동안 대응안하고 뭐했나?

각 타임라인을 보면서 이런 의문점에 대해서 하나씩 답변드리겠습니다.

> 저희의 면죄부를 이야기한다기 보다는, 하나의 작은 스타트업이 기술 부채로 인해 발생한 서비스 장애의 원인을 어떻게 찾고, 이를 해결하기 위한 임시 방안, 전반적인 근본 원인을 조사하는 과정을 소개드리는 것으로 생각해주시면 좋을것 같습니다.
 
# 2. 장애 회고

장애 회고에 대해 말씀드리기 앞서, 현재 인프런 서비스의 구조에 대해서 짧게 소개해드리겠습니다.

## 서버 구조

![system](./images/system.png)

> Application Load Balancer, ElasticCache (Redis), DynamoDB, AWS Lambda 등도 사용중이지만, 이번 장애와는 무관하기 때문에 위 그림에서는 제외했습니다.

현재 인프런의 경우 **모놀리틱 NodeJS 프로젝트**를 ECS Fargate 에 올려 트래픽에 따라 **10대 ~ 30대**를 유지하고 있습니다.  
이번 장애와 직접적인 영향이 있는 데이터베이스의 경우 **AWS Aurora PostgreSQL 10.16**을 사용중인데요.  

* 1대의 Writer (Master) 
* 2 ~ 7대의 Reader (Replacation) 

대부분의 조회용 쿼리는 Reader DB를 통해 처리하고, 등록/수정/삭제의 요청들은 Writer DB를 통해 처리하면서 적절하게 DB 부하를 분산하고 있었습니다.  

> [Multi Master](https://aws.amazon.com/ko/blogs/korea/building-highly-available-mysql-applications-using-amazon-aurora-mmsr/) 의 경우 아직까지 고려하고 있지는 않습니다.  
> 애초에 서비스별로 별도 DB를 구성하는게 가용성과 성능에 더 효과적이라고 생각하기 때문입니다.

서비스별로 각자의 DB를 갖고 있지 않은 상태이기 때문에, **한 서비스의 장애는 전체 서비스의 장애**로 확장될 수 밖에 없는 구조입니다.  

> 물론 그렇기 때문에 MSA (MicroService Architecture)로 가야한다는 것이 이번 글의 핵심은 아닙니다.  

적절한 서버 대수 (10대 ~ 30대) 와 DB 부하 분산 (총 8대의 DB / Replication 구조) 가 되어있음에도 장애를 만나게 된 이유가 있는데요.   
  
위 내용을 기반으로 지난 1월 3일부터 1월 17일까지 발생했던 서버 문제를 살펴보며 어떤 상황이 발생하고 조치를 취했는지 정리해보겠습니다.

## 1월 3일 장애

1월 3일 20:00부터 이벤트를 통한 트래픽이 발생하면서 20:30분부터 22:00 까지 1시간 30분동안 서비스가 사용 불가능

![2](./images/2.png)

관심회사 조회 (a.k.a 메로빈지언) API 슬로우

시간: 2022.01.03 20:30 ~  2022.01.03 22:00 (1시간 30분)

원인: 인덱스를 제대로 타지 못해 발생한 슬로우 쿼리

장애내용

기존에 인덱스가 적용 안되는 관심회사 조회 API가 트래픽이 급증하면서 기존보다 대량으로 발생

해당 슬로우 API들로 인해 Node 서버 전체가 전체적인 지연 현상 발생



(당시 DB의 CPU는 10~30%, 세션은 10개가 안되었던 상황)

 

해결
해당 쿼리에 대한 인덱스 추가 하여 슬로우 쿼리 해소

[Join] 조인 & 추가 조건절 함께 필요한 경우 

추가 이슈

DB 쿼리가 튜닝되었으나, Node 애플리케이션이 요청수를 못버텨 프로세스가 강제 종료

ECS 컨테이너수를 추가 투입으로 해소

대용량의 데이터 (cache 테이블)의 잦은 IO 이슈

시간: 2022.01.03 23:30 ~  2022.01.04 01:30 (2시간)

원인: Write DB에 대한 부하 & 테이블 데드락

장애내용

엄청나게 큰 cache 데이터의 Insert 와 아직 Reader DB로 전환하지 못한 조회 쿼리들이 합쳐져 CPU 90% 이상까지 올라가서 Write DB 사망



 

이로 인해 전체 서비스가 장애

Write DB 사양 업그레이드 & 재부팅으로 초기화

DB 복구

DB가 복구됨에 따라 다시 트래픽 증가

Node 서버들이 트래픽을 버티지 못해 추가로 30대의 서버를 동시 투입

추가 투입되는 서버들간에 Cache Delete Deadlock 발생

아래와 같이 caches 테이블을 30대가 동시에 동일한 Row를 Delete 요청하여 DeadLock 발생 



RDS DeadLock 지표
이로인해 서비스가 실행을 못하고 대기 상태에 빠지다 바로 종료 되는 현상 발생


아예 서비스가 실행도 못되어서 APM 추적조차 안되었던 상황
 

해결
상세 작업 내용

인프런 배포후 실행되는 cache 테이블 clear 하는 로직 제거

Write DB에서 수행하는 조회 쿼리 제거

업데이트 쿼리에 사용된 뷰 테이블을 제거하고, 집계 방식이 아닌 +1, -1 방식으로 변경

## 1월 4일 장애

course 테이블 조회 슬로우로 인한 서비스 장애

시간: 2022.01.04 09:00 ~  2022.01.04 12:00 (3시간)

원인: 성능 개선이 되지 못한 수많은 course 조회 쿼리들의 누적으로 인한 서비스 장애

장애내용


해결
불필요한 쿼리 요청들 제거

페이스북으로 보내는 마케팅 스크립트 주석 처리

메인페이지에서 사용중인 courses 관련 탭 제거

로그아웃 유저 

입문강의 리스트

로그인 유저

내가 수강중인 강의의 스킬태그 기반 추천강의 

인덱스를 제대로 타지 못하는 쿼리들 개선

lower("slug") = lower(${slug}) => "slug" = lower(${slug})

내가 구매한 강의 필터 기능 제거

고용량의 select 데이터 조회 (courses 테이블)  및 그로 인한 autovaccum 부하 

시간: 2022.01.04 21:00 ~  2022.01.05 01:40 (4시간 40분)

원인:

 body, text_body 와 같이 강의 상세 정보 (HTML 코드까지 모두 담긴) 가 담긴 고용량의 컬럼을 페이징 쿼리 조회 항목에서 사용

autovaccum 부하가 발생하여 DB 부하 발생

장애내용

리더 DB에서 조회 지연 현상 발생

아래와 같이 페이징으로 수십개씩 조회하는 쿼리에서 (사용하지도 않는) 고용량의 컬럼 (body, text_body) 포함한 형태로 조회

```sql
SELECT "courses".*
FROM "courses" AS "courses"
WHERE (("id" IN (SELECT "course_id" FROM "courses_tags" WHERE ("term_id" = $1)) AND
        ("status" = ? AND "published_date" < now() AND "deleted_at" IS NULL) AND "exposure" = $2) AND
       "deleted_at" IS NULL)
ORDER BY "total_seq" DESC, "id" DESC
LIMIT $3 OFFSET $4;
```

이로 인해 DB 부하 발생


그리고 이를 통해 vaccum 부하가 발생하게 됨


courses 테이블의 vaccum 부하로 인해 DB Load가 계속 올라가게 되어 전체 테이블 조회 지연이 양산됨

해결
courses 테이블의 Autovaccum 종료

ALTER TABLE public.courses SET (autovacuum_enabled = false);

DB 지연 해소 이후 다시 Autovaccum 복구

29cm 장애 해결 사례

select * 로 조회하던 course 쿼리를 아래와 같이 모두 body와 text_body를 제외하고 직접 컬럼 지정하는 방식으로 해소

메인페이지

강의리스트

강의 상세 유사강의 페이지

pages

카트 페이지에서 쿠폰 적용 가능한 강의 조회

태그큐레이션페이지


주의)
4차 장애는 시스템의 문제 보다 우리가 자초한 문제가 3개가 발생해 오히려 더 치명적이다.

코드 수정후 사람이 검증하지 않으면 타입체크도, 기능 검증도 전혀 안되서 배포후 버그가 발생할 확률이 높다

그래서 다시 롤백하고 배포 대기시간동안 대기 했던 것

한번 잘못배포되고 나니 다음 배포까지 또 20분씩 (배포 시간) 장애나는 상황을 지켜만 보고 있어야 했던 상황

슬로우 쿼리가 뭔지 알아도, 어디를 고쳐야하는지 알 수 없는 ORM 코드들

 

1. 아래와 같이 최종 쿼리가 어떤 형태인지 예측할 수 없는 형태로 이루어져있어, 슬로우 쿼리가 뭔지 아는 상황에서도 그게 어느 페이지 혹은 어느 코드에서 쓰이는지를 못찾던 상황

후리 / 꾸기 / 비스타 3명이서 7개 쿼리 수정하는데 3시간이 걸렸다.

해당 쿼리가 어느 페이지 / 어느 코드에서 호출되는지 찾는데만 걸린 것이다.



 

2. 급한 마음에 로컬 환경 테스트 없이 배포했다가 강의 검색 페이지 버그가 있는 상태로 배포


기본적인 기능검증과 타입체크조차 지원되지 않아 배포하기전에 개발자가 테스트하는걸 놓치면 배포하자마자 장애가 발생한다. 

 

## 1월 7일 장애 

장바구니, 구매완료, 결제페이지 슬로우 쿼리

시간: 2022.01.07 21:30 ~  2022.01.03 22:00 (1시간 30분)

원인: where id in () 에 100개 이상의 ID를 넣어서 인덱스 풀스캔이 발생하여 서비스 장애

결제시 주문서 생성 과정에서 장바구니에 담긴 강의 정보를 가져오고 있음

현재 이벤트 진행중인 무료 강의 125개 한번에 담아서 in 절에 해당 강의들이 모두 담긴 쿼리가 실행


 

장애내용

course 관련 조회 쿼리에서 대량의 3초이상 슬로우 쿼리 발생

Reader DB 부하 심화

select * from course 와 관련된 쿼리들 body, text_body 컬럼 제거

슬로우 쿼리 감소

앞서 Reader DB 응답 지연으로 Node JS 애플리케이션 좀비 프로세스

헬스체크시 DB 상태 체크 로직을 제외해서 Node 자체가 죽지는 않음

다만, DB가 복구되어도 Node와 DB간 Connection이 재연결 안됨

NodeJS 서버들 전체 재투입


해결
120건씩 in 에 담기는 것들은 모두 20건씩 나눠 담아서 병렬 쿼리 수행

후속조치
결제페이지(카트리스트 API 포함), 결제완료페이지, 결제내역상세페이지, 대시보드페이지, 내 수강 목록페이지 등에서
다른 데이터(cart/voucher)로 시작하여 id 기반으로 강의를 조회하는 쿼리에 body, text_body 컬럼 제외

장바구니 페이지, 주문서 생성 API, 구매완료 페이지 등에서 조회되는 강의 데이터 컬럼 최적화

수강권 생성시 강의 수강생수 업데이트 로직을 트랜잭션 묶음에서 제외

수강권 생성 후 진도율 동기화 로직 생략



## 1월 17일 장애 (최종)

현재 (2022.01.15 토요일 오전 09시 ~ ) 이번주 가장 많은 트래픽이 발생하고 있습니다.
(목/금 저녁 10시보다 보다 더)
그럼에도 시스템 모니터링 상에서 이슈되는 영역은 없습니다

![final_0](./images/final_0.png)

그래서 문제가 해결되었구나 라는 생각을 하고 장기 개선 계획만 세우고 있었습니다.  

![final_1](./images/final_1.png)

### 근본 원인 분석

**1만건도 되지 않는 강의 테이블만을 탐색**하는 쿼리에서도 슬로우가 발생한 것입니다.  

![final_2](./images/final_2.png)

![final_3](./images/final_3.png)

![final_4](./images/final_4.png)


해당 강의 테이블만 `vacuum` 이 지속적으로 발생하고 있었습니다.  
그리고 

* [a-beginners-guide-to-postgresqls-update-and-autovacuum](https://www.cybertec-postgresql.com/en/a-beginners-guide-to-postgresqls-update-and-autovacuum/)

postgresql은 update가 실질적으로 기존 row는 그대로 두고, 새 row를 insert 하는 방식인데,
수강생수 업데이트 때문에 (구) row가 너무 빈번하게 쌓이고 있습니다
(count+1할때마다 기존 row는 디스크조각모음때 삭제하기 위해 남겨놓고, 새 row 넣는 방식)
그래서 계속 디스크조각모음이 발생하고,
그게 한계치에 이르면 지금처럼 모든 쿼리가 슬로우가 되는데요

# 마무리

